<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="keywords" content="c++, python, java, html5, css3, 编程, 学生编程，个人博客，大学生博客"><meta name="description" content="目前是大二学生，没有可以拿出来说道的东西，慢慢学，慢慢走"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><title>python爬虫:带你游览微博博主的前世今生 | 落阳的博客</title><link rel="shortcut icon" href="/favicon.ico"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css"><script src="/js/pace.min.js"></script><meta name="generator" content="Hexo 4.2.0"></head></html><body><div id="app"><main class="content"><section class="outer"><article id="post-python爬虫-带你游览微博博主的前世今生" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal><div class="article-inner"><header class="article-header"><h1 class="article-title sea-center" style="border-left:0" itemprop="name">python爬虫:带你游览微博博主的前世今生</h1></header><div class="article-meta"><a href="/python%E7%88%AC%E8%99%AB-%E5%B8%A6%E4%BD%A0%E6%B8%B8%E8%A7%88%E5%BE%AE%E5%8D%9A%E5%8D%9A%E4%B8%BB%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F.html" class="article-date"><time datetime="2020-03-14T05:15:38.000Z" itemprop="datePublished">2020-03-14</time></a></div><div class="tocbot"></div><div class="article-entry" itemprop="articleBody"><h3 id="目录概览"><a href="#目录概览" class="headerlink" title="目录概览"></a>目录概览</h3><p><strong>一、 前言<br>二、 项目目标<br>三、 环境配置<br>四、 数据提取分析<br>五、 代码编写<br>六、 结果展示<br>七、 项目总结</strong></p><h3 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h3><hr><p>因为疫情的缘故，最近在家老被疫情微博消息轰炸，还每次都忍不住点进去看，关心国内又增长了多少人出院了多少人，国外，尤其是韩国日本伊朗等又激增了多少人，然后看下面大家的评论，看的我胆战心惊的。疫情不分国界，希望大家都能顺顺利利挺过这次全球灾难。</p><p>当然，被困在家也要找点事情做，目前在研究爬虫，因为上面提到的微博的事，刚好就把目标放到微博上来了。</p><p>接下来我们就一起来爬取微博数据吧！</p><a id="more"></a><h3 id="二、项目目标"><a href="#二、项目目标" class="headerlink" title="二、项目目标"></a>二、项目目标</h3><ol><li>任给一个用户的微博主页链接能够爬取他所有的微博以及点赞数等相关信息。</li><li>对任意一条微博，可以爬取到他所有的回复，以及回复的点赞数，被回复数。</li><li>将以上两者结合起来，实现对任意一个用户，爬取他的微博信息，所有微博以及每条微博的所有评论信息。</li><li><strong>额外拓展(尚未完成):</strong> 对每条微博的评论内容再做一次提取，提取评论者id，然后进入评论者主页进行重复爬取，直至完成对整个微博所有用户所有信息的爬取。</li></ol><h3 id="三、环境配置"><a href="#三、环境配置" class="headerlink" title="三、环境配置"></a>三、环境配置</h3><ul><li>语言：python 3.8.1</li><li>开发工具：vscode</li><li>浏览器：Chrome</li><li>抓包工具：mimtweb</li><li>爬虫框架：scrapy 1.8.0</li></ul><h3 id="四、数据提取分析"><a href="#四、数据提取分析" class="headerlink" title="四、数据提取分析"></a>四、数据提取分析</h3><h4 id="4-1-用户微博主页分析"><a href="#4-1-用户微博主页分析" class="headerlink" title="4.1 用户微博主页分析"></a>4.1 用户微博主页分析</h4><p>从电脑进入到微博网站，注意，这里进的是 w.weibo.cn 这个网址，这个是移动端的网址，界面看起来简单很多，少很多干扰因素。</p><p>进入到用户微博主页，为了防止你们说我打广告，我进是团团的主页️️<br><img src="https://img-blog.csdnimg.cn/20200305002843389.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x1b3lhbmdJVA==,size_16,color_FFFFFF,t_70" alt="用户微博主页"></p><p>经过chrome的自带工具，我们可以看到这个html页面文件里面什么数据也没有，所以我们判断这个页面的数据是异步加载请求的。我们捕捉这个请求。<br><img src="https://img-blog.csdnimg.cn/20200305093651981.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x1b3lhbmdJVA==,size_16,color_FFFFFF,t_70" alt="捕捉异步请求"><br>对请求进行一一分析之后，找到了这个请求微博用户信息的接口，它返回的是标准的json格式数据。里面除了用户信息还有一些其它的数据，我们暂时不知道有什么用，先留着。</p><p>接下来我们需要对接口进行优化，这样做是为了提高爬虫效率同时避免因为一些不必要的参数导致请求失败。</p><p>我们来看优化前后的请求对比️<br><img src="https://img-blog.csdnimg.cn/20200305100322891.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x1b3lhbmdJVA==,size_16,color_FFFFFF,t_70" alt="优化前的请求"><br>对比优化之后的请求️<br><img src="https://img-blog.csdnimg.cn/20200305100640872.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x1b3lhbmdJVA==,size_16,color_FFFFFF,t_70" alt="优化之后的请求"><br>经过一番折腾，发现对于这个请求而言，url链接里面的containerid是不必要的，可以删去，同时请求头里面很多参数也是不必要的。cookie里面唯一需要保留的就是这个SUB值，如果失去了它，会请求失败。</p><p>优化之后我们可以利用用户id来拼凑出请求它主页信息的接口url。<br><img src="https://img-blog.csdnimg.cn/20200305145818506.png" alt="主页url"><br>这是主页的url，<strong>u/</strong> 后面那一部分就是用户的id。用这个id拼凑出接口，去掉后面那个containerid值。<br><img src="https://img-blog.csdnimg.cn/20200305114644833.png" alt="拼凑接口"></p><p>现在微博用户信息的接口找到并优化好之后，就需要开始寻找请求这个微博用户的每一条微博的接口。<br><img src="https://img-blog.csdnimg.cn/20200305102754487.png" alt="请求接口"><br>寻找到了请求微博的接口，但是这部分出现了一个有意思的东西！来观察一下这个新找到的接口。<br><img src="https://img-blog.csdnimg.cn/20200305103002728.png" alt="接口"><br>惊讶的发现，这个接口居然和刚才那个接口惊人的相似！经过分析之后，发现这两个接口之间只有一个地方不同！那就是url后面的<strong>containerid</strong>值。当尝试把这个containerid值删去之后，请求回来的结果果然又变成了之前请求微博用户信息的结果。</p><p>然后先将微博往下翻，让他继续请求新的微博信息，得到这样一个url<br><img src="https://img-blog.csdnimg.cn/20200305110431461.png" alt="新微博信息url"></p><p>与第一次请求微博相比，它又只多了一个参数<strong>since_id</strong>，并且containerid不变，这样的话，我们可以把它理解成起始量，也就是，从哪开始获取新的微博信息。</p><p>接下来的任务是寻找到 <strong>containerid</strong> 和 <strong>since_id</strong>的值是从哪获得的。</p><p>山重水复疑无路，踏破铁鞋无觅处。柳暗花明又一村，得来全不费功夫。（狗头）<strong>containerid</strong> 值就存放在刚开始请求用户信息的地方。<br><img src="https://img-blog.csdnimg.cn/20200305112501338.png" alt="containerid值"><br>看到这一部分内容，再联想到container这个单词，便可大致理解它为一个容器，所以这个id就是专门存储微博的容器id。</p><p>然后联想到since_id的作用，它是用来标明这一次请求微博从哪里开始，那么我们应该能在上一次请求微博返回的信息中找到它，不出我的所料️️<br><img src="https://upload-images.jianshu.io/upload_images/20361402-36a0d678d7ee1ebc" alt="since_id位置"></p><p>然后同样的，优化一下请求微博的接口参数，用户微博主页分析我们就算完成了，来小结一下请求步骤。</p><ol><li>获取用户主页url，获得用户id。</li><li>利用用户id拼凑出请求微博用户信息的接口。</li><li>获取需要的用户信息，并获取微博的containerid值。</li><li>再利用上一个接口和containerid值拼凑出请求微博的第一个接口url。</li><li>获取微博信息之后，利用里面的since_id再拼凑出第二次请求微博的接口url。</li><li>重复第五步直到抓取完毕。</li></ol><h4 id="4-2-微博详情页分析"><a href="#4-2-微博详情页分析" class="headerlink" title="4.2 微博详情页分析"></a>4.2 微博详情页分析</h4><p>进入一个微博的详情页，简单分析了一下数据来源，发现在详情页里面的微博文本虽然没有直接放在html元素里面呈现出来，但其实并不是异步请求。而是放在了html文件里的js代码内部封装️️<br><img src="https://img-blog.csdnimg.cn/202003051318579.png" alt="详情页内容"><br>我们可以通过正则从html文件中提取出微博的文本数据。</p><p>接下来再寻找评论部分的数据来源。</p><p>评论的接口网址是这个<br><img src="https://img-blog.csdnimg.cn/2020030514302954.png" alt="评论接口"><br>这个接口中的id和mid数值一样且固定为这条微博的id，而这条微博的id可以从4.1中获取或者是微博详情页URL获取。</p><p>这样就可以拼凑出第一批评论接口url。</p><p><img src="https://img-blog.csdnimg.cn/20200305142323559.png" alt="评论数据来源"><br>这就是第一批评论的数据来源接口了，为什么非要强调这是第一批呢？因为从第二批开始，接口就有所变化了。看一下对比。<br><img src="https://img-blog.csdnimg.cn/20200305142521403.png" alt="第一批"><br>️️️<br><img src="https://img-blog.csdnimg.cn/20200305142621755.png" alt="第一批之后"><br>从这里看出来，从第二批评论开始，接口中就多了一个参数<strong>max_id</strong>，而经过抓包修改测试，这个参数无法去除，同时数值也需要准确，会不停变化。</p><p>那么这个max_id从哪来呢？</p><p>还记得4.1 的时候分析的那个数值since_id嘛？从前一个接口里面获取到下一个接口需要的参数。这里也是一样的道理！</p><p>来看一下通过获取第一批评论的接口获取到的数据下方：<br><img src="https://img-blog.csdnimg.cn/20200305143405189.png" alt="数据"></p><p>果不其然这里有我们需要的max_id，这样我们就能很简单的拼凑出再下一批的接口url了。</p><p>等一下。你以为这样就算完了？</p><p>不！经过我的踩坑，这里还有一个很需要注意的地方，就是那个不起眼的<strong>max_id_type</strong>。</p><p>在上面的接口url中，它一直都等于0，但是事实上，它是会变成1的。并且暂时没有摸清具体什么时候变。</p><p>这个坑，如果踩过就很简单，因为max_id_type的值也是与max_id一同知道了的，但是如果没踩过，很容易误认为就永远为0。</p><p>优化一下请求的接口参数：<img src="https://img-blog.csdnimg.cn/20200305144014874.png" alt="优化评论接口"><br>这里的cookie同样只需要SUB，值与4.1相同且不变，如果失去这个SUB会被重定向导致获取不到数据。</p><p>小结一下4.2:</p><ol><li>从微博详情页html里面用正则提取出文本内容。</li><li>利用微博id拼凑出第一批评论请求接口。</li><li>从第一批接口中提取数据，同时利用获取的max_Id 和max_id_type拼凑下一个接口。</li><li>获取数据，拼凑下一个接口。</li><li>重复第四步。</li></ol><h3 id="五、代码编写"><a href="#五、代码编写" class="headerlink" title="五、代码编写"></a>五、代码编写</h3><p>到了手底下见真章的时候了。开始吧。</p><h4 id="5-1-创建scrapy项目和爬虫"><a href="#5-1-创建scrapy项目和爬虫" class="headerlink" title="5.1 创建scrapy项目和爬虫"></a>5.1 创建scrapy项目和爬虫</h4><p>不用使用-t crawl模板设置规则来爬去，直接创建一个普通的爬虫就可以</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject weibo</span><br><span class="line">cd weibo</span><br><span class="line">scrapy genspider one_people</span><br></pre></td></tr></table></figure><p>原谅我粗糙的取名水平。</p><h4 id="5-2-修改setting-py"><a href="#5-2-修改setting-py" class="headerlink" title="5.2 修改setting.py"></a>5.2 修改setting.py</h4><ul><li>robots协议</li><li>爬取延迟</li><li>默认请求头，关闭cookie</li><li>打开管道和下载中间件</li></ul><p>首先把遵守robot协议设置为False，同时把爬取延时设置三秒以上</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ROBOTSTXT_OBEY &#x3D; False</span><br><span class="line"></span><br><span class="line"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span><br><span class="line">#CONCURRENT_REQUESTS &#x3D; 32</span><br><span class="line"></span><br><span class="line"># Configure a delay for requests for the same website (default: 0)</span><br><span class="line"># See https:&#x2F;&#x2F;docs.scrapy.org&#x2F;en&#x2F;latest&#x2F;topics&#x2F;settings.html#download-delay</span><br><span class="line"># See also autothrottle settings and docs</span><br><span class="line">DOWNLOAD_DELAY &#x3D; 3</span><br></pre></td></tr></table></figure><p>然后设置一下默认的请求头，并且有很重要的一点是：<strong>将cookies设置为禁用状态</strong>。</p><p>因为如果不禁用，那么scrapy框架会根据返回的<strong>set-cookie</strong>值自动生成cookie，最后导致网页被重定向。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># Disable cookies (enabled by default)</span><br><span class="line">COOKIES_ENABLED &#x3D; False</span><br><span class="line"></span><br><span class="line"># Disable Telnet Console (enabled by default)</span><br><span class="line">#TELNETCONSOLE_ENABLED &#x3D; False</span><br><span class="line"></span><br><span class="line"># Override the default request headers:</span><br><span class="line">DEFAULT_REQUEST_HEADERS &#x3D; &#123;</span><br><span class="line">    &#39;host&#39;:	&#39;m.weibo.cn&#39;,</span><br><span class="line">    &#39;accept&#39;:	&#39;application&#x2F;json, text&#x2F;plain, *&#x2F;*&#39;,</span><br><span class="line">    &#39;user-agent&#39;:	&#39;Mozilla&#x2F;5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;80.0.3987.122 Safari&#x2F;537.36&#39;,</span><br><span class="line">    &#39;accept-encoding&#39;:	&#39;gzip, deflate, br&#39;,</span><br><span class="line">    &#39;accept-language&#39;:	&#39;zh-CN,zh;q&#x3D;0.9&#39;,</span><br><span class="line">    &#39;cookie&#39;:	&#39;SUB&#x3D;_2A25zUZngDeRhGeBO6FQW9izFyjuIHXVQvSeorDV6PUNbktANLXPVkW1NShqrqT_gNAKqD3jr0wVYJ8UqOFgnZdeJ;&#39;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后就是把下载中间件和管道开起来。下载中间件用来随机更换请求头，有必要的话也用来更换ip， 管道用来存储数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES &#x3D; &#123;</span><br><span class="line">    &#39;weibo.middlewares.WeiboDownloaderMiddleware&#39;: 543,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Enable or disable extensions</span><br><span class="line"># See https:&#x2F;&#x2F;docs.scrapy.org&#x2F;en&#x2F;latest&#x2F;topics&#x2F;extensions.html</span><br><span class="line"># EXTENSIONS &#x3D; &#123;</span><br><span class="line">#    &#39;scrapy.extensions.telnet.TelnetConsole&#39;: None,</span><br><span class="line"># &#125;</span><br><span class="line"></span><br><span class="line"># Configure item pipelines</span><br><span class="line"># See https:&#x2F;&#x2F;docs.scrapy.org&#x2F;en&#x2F;latest&#x2F;topics&#x2F;item-pipeline.html</span><br><span class="line">ITEM_PIPELINES &#x3D; &#123;</span><br><span class="line">    &#39;weibo.pipelines.WeiboPipeline&#39;: 300,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="5-3-设置items-py"><a href="#5-3-设置items-py" class="headerlink" title="5.3 设置items.py"></a>5.3 设置items.py</h4><p>为三种爬取的数据聚合设置三个item</p><p>item的编写是确定我们要爬取的数据内容，然后可以封装在scrapy.Item类里面从爬虫部分输送到管道部分保存。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">class CommentItem(scrapy.Item):</span><br><span class="line">    &#39;&#39;&#39;评论item&#39;&#39;&#39;</span><br><span class="line">    # 评论时间</span><br><span class="line">    comment_time &#x3D; scrapy.Field()</span><br><span class="line">    # 评论文本</span><br><span class="line">    text &#x3D; scrapy.Field()</span><br><span class="line">    # 评论人id</span><br><span class="line">    comment_people_id &#x3D; scrapy.Field()</span><br><span class="line">    # 评论人name</span><br><span class="line">    comment_people_name &#x3D; scrapy.Field()</span><br><span class="line">    # 评论点赞数</span><br><span class="line">    comment_likes &#x3D; scrapy.Field()</span><br><span class="line">    # 评论回复总数</span><br><span class="line">    total_number &#x3D; scrapy.Field()</span><br><span class="line"></span><br><span class="line">class PeopleItem(scrapy.Item):</span><br><span class="line">    &#39;&#39;&#39;用户item&#39;&#39;&#39;</span><br><span class="line">    # 用户昵称</span><br><span class="line">    name &#x3D; scrapy.Field()</span><br><span class="line">    # 用户id</span><br><span class="line">    user_id &#x3D; scrapy.Field()</span><br><span class="line">    # 关注数</span><br><span class="line">    follow_count &#x3D; scrapy.Field()</span><br><span class="line">    # 粉丝数</span><br><span class="line">    followers_count &#x3D; scrapy.Field()</span><br><span class="line">    # 描述</span><br><span class="line">    description &#x3D; scrapy.Field()</span><br><span class="line">    # 微博数</span><br><span class="line">    statuses_count &#x3D; scrapy.Field()</span><br><span class="line">    # 是否认证</span><br><span class="line">    verified &#x3D; scrapy.Field()</span><br><span class="line">    # 认证缘由</span><br><span class="line">    verified_reason &#x3D; scrapy.Field()</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">class StatusesItem(scrapy.Item):</span><br><span class="line">    &#39;&#39;&#39;微博item&#39;&#39;&#39;</span><br><span class="line">    # 最后编辑于</span><br><span class="line">    edit_at &#x3D; scrapy.Field()</span><br><span class="line">    # 文本</span><br><span class="line">    text &#x3D; scrapy.Field()</span><br><span class="line">    # 转发数</span><br><span class="line">    reposts_count &#x3D; scrapy.Field()</span><br><span class="line">    # 评论数</span><br><span class="line">    comments_count &#x3D; scrapy.Field()</span><br><span class="line">    # 点赞数</span><br><span class="line">    attitudes_count &#x3D; scrapy.Field()</span><br><span class="line">    # 微博id</span><br><span class="line">    statues_id &#x3D; scrapy.Field()</span><br><span class="line">    # 详情页URL</span><br><span class="line">    origin_url &#x3D; scrapy.Field()</span><br></pre></td></tr></table></figure><h4 id="5-4-编写one-people-py"><a href="#5-4-编写one-people-py" class="headerlink" title="5.4 编写one_people.py"></a>5.4 编写one_people.py</h4><p>长代码警告</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">import json</span><br><span class="line">from weibo.items import PeopleItem, StatusesItem, CommentItem</span><br><span class="line">import re</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class OnePeopleSpider(scrapy.Spider):</span><br><span class="line">    name &#x3D; &#39;one_people&#39;</span><br><span class="line">    allowed_domains &#x3D; [&#39;w.weibo.cn&#39;]</span><br><span class="line">    start_urls &#x3D; [&#39;https:&#x2F;&#x2F;m.weibo.cn&#x2F;u&#x2F;3664122147&#39;]</span><br><span class="line">    usr_id &#x3D; start_urls[0].split(&#39;&#x2F;&#39;)[-1]</span><br><span class="line"></span><br><span class="line">    def start_requests(self):</span><br><span class="line">        &#39;&#39;&#39;首先请求第一个js文件，包含有关注量，姓名等信息&#39;&#39;&#39;</span><br><span class="line">        js_url &#x3D; &#39;https:&#x2F;&#x2F;m.weibo.cn&#x2F;api&#x2F;container&#x2F;getIndex?type&#x3D;uid&amp;value&#x3D;&#39; + \</span><br><span class="line">            self.usr_id</span><br><span class="line">        yield scrapy.Request(url&#x3D;js_url,</span><br><span class="line">                             callback&#x3D;self.parse_info,</span><br><span class="line">                             dont_filter&#x3D;True)</span><br><span class="line"></span><br><span class="line">    def parse_info(self, response):</span><br><span class="line">        js &#x3D; json.loads(response.text)</span><br><span class="line">        infos &#x3D; js[&#39;data&#39;][&#39;userInfo&#39;]</span><br><span class="line">        name &#x3D; infos[&#39;screen_name&#39;]</span><br><span class="line">        user_id &#x3D; infos[&#39;id&#39;]</span><br><span class="line">        follow_count &#x3D; infos[&#39;follow_count&#39;]</span><br><span class="line">        followers_count &#x3D; infos[&#39;followers_count&#39;]</span><br><span class="line">        description &#x3D; infos[&#39;description&#39;]</span><br><span class="line">        # 微博数</span><br><span class="line">        statuses_count &#x3D; infos[&#39;statuses_count&#39;]</span><br><span class="line">        verified &#x3D; infos[&#39;verified&#39;]</span><br><span class="line">        verified_reason &#x3D; &#39;&#39;</span><br><span class="line">        if verified &#x3D;&#x3D; True:</span><br><span class="line">            verified_reason &#x3D; infos[&#39;verified_reason&#39;]</span><br><span class="line">        item &#x3D; PeopleItem(name&#x3D;name,</span><br><span class="line">                          user_id&#x3D;user_id,</span><br><span class="line">                          follow_count&#x3D;follow_count,</span><br><span class="line">                          followers_count&#x3D;followers_count,</span><br><span class="line">                          description&#x3D;description,</span><br><span class="line">                          statuses_count&#x3D;statuses_count,</span><br><span class="line">                          verified&#x3D;verified,</span><br><span class="line">                          verified_reason&#x3D;verified_reason)</span><br><span class="line">        yield item</span><br><span class="line"></span><br><span class="line">        weibo_containerid &#x3D; str(</span><br><span class="line">            js[&#39;data&#39;][&#39;tabsInfo&#39;][&#39;tabs&#39;][1][&#39;containerid&#39;])</span><br><span class="line">        con_url &#x3D; &#39;&amp;containerid&#x3D;&#39; + weibo_containerid</span><br><span class="line">        next_url &#x3D; response.url + con_url</span><br><span class="line">        print(next_url)</span><br><span class="line">        yield scrapy.Request(url&#x3D;next_url,</span><br><span class="line">                             callback&#x3D;self.parse_wb,</span><br><span class="line">                             dont_filter&#x3D;True)</span><br><span class="line"></span><br><span class="line">    def parse_wb(self, response):</span><br><span class="line">        try:</span><br><span class="line">            js &#x3D; json.loads(response.text)</span><br><span class="line">            datas &#x3D; js[&#39;data&#39;][&#39;cards&#39;]</span><br><span class="line">            for data in datas:</span><br><span class="line">                # 去掉推荐位和标签位</span><br><span class="line">                if len(data) &#x3D;&#x3D; 4 or &#39;mblog&#39; not in data:</span><br><span class="line">                    continue</span><br><span class="line">                edit_at &#x3D; data[&#39;mblog&#39;][&#39;created_at&#39;]</span><br><span class="line">                text &#x3D; data[&#39;mblog&#39;][&#39;text&#39;]</span><br><span class="line">                reposts_count &#x3D; data[&#39;mblog&#39;][&#39;reposts_count&#39;]</span><br><span class="line">                comments_count &#x3D; data[&#39;mblog&#39;][&#39;comments_count&#39;]</span><br><span class="line">                attitudes_count &#x3D; data[&#39;mblog&#39;][&#39;attitudes_count&#39;]</span><br><span class="line">                statues_id &#x3D; str(data[&#39;mblog&#39;][&#39;id&#39;])</span><br><span class="line">                origin_url &#x3D; data[&#39;scheme&#39;].split(&#39;?&#39;)[0]</span><br><span class="line"></span><br><span class="line">                item &#x3D; StatusesItem(edit_at&#x3D;edit_at,</span><br><span class="line">                                    text&#x3D;text,</span><br><span class="line">                                    reposts_count&#x3D;reposts_count,</span><br><span class="line">                                    comments_count&#x3D;comments_count,</span><br><span class="line">                                    attitudes_count&#x3D;attitudes_count,</span><br><span class="line">                                    statues_id&#x3D;statues_id,</span><br><span class="line">                                    origin_url&#x3D;origin_url)</span><br><span class="line">                yield item</span><br><span class="line">            if &#39;since_id&#39; not in js[&#39;data&#39;][&#39;cardlistInfo&#39;]:</span><br><span class="line">                exit(0)</span><br><span class="line">            since_id &#x3D; str(js[&#39;data&#39;][&#39;cardlistInfo&#39;][&#39;since_id&#39;])</span><br><span class="line">            next_url &#x3D; &#39;&#39;</span><br><span class="line">            if &#39;since_id&#39; not in response.url:</span><br><span class="line">                next_url &#x3D; response.url + &#39;&amp;since_id&#x3D;&#39; + since_id</span><br><span class="line">            else:</span><br><span class="line">                next_url &#x3D; re.sub(r&#39;since_id&#x3D;\d+&#39;, &#39;since_id&#x3D;%s&#39; %</span><br><span class="line">                                  since_id, response.url)</span><br><span class="line">        except Exception as ret:</span><br><span class="line">            print(&quot;&#x3D;&quot; * 40)</span><br><span class="line">            print(&quot;这里出错了: %s&quot; % ret)</span><br><span class="line">            print(&quot;&#x3D;&quot;*40)</span><br><span class="line">            print(js)</span><br><span class="line">            print(&quot;&#x3D;&quot; * 40)</span><br><span class="line">        yield scrapy.Request(url&#x3D;next_url,</span><br><span class="line">                             callback&#x3D;self.parse_wb,</span><br><span class="line">                             dont_filter&#x3D;True)</span><br><span class="line">        </span><br><span class="line">        self.comments_url &#x3D; &#39;https:&#x2F;&#x2F;m.weibo.cn&#x2F;comments&#x2F;hotflow?id&#x3D;&#123;0&#125;&amp;mid&#x3D;&#123;1&#125;&#39;.format(statues_id, statues_id)</span><br><span class="line">        yield scrapy.Request(url&#x3D;self.comments_url,</span><br><span class="line">                             callback&#x3D;self.parse_comments,</span><br><span class="line">                             dont_filter&#x3D;True)</span><br><span class="line"></span><br><span class="line"># &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"># 下面这部分爬取每条微博的评论，</span><br><span class="line">    def parse_comments(self, response):</span><br><span class="line">        js &#x3D; json.loads(response.text)</span><br><span class="line">        max_id &#x3D; &#39;&amp;max_id&#x3D;&#39; + str(js[&#39;data&#39;][&#39;max_id&#39;])</span><br><span class="line">        next_url &#x3D; response.url + max_id</span><br><span class="line">        print(&quot;&#x3D;&quot; * 40)</span><br><span class="line">        print(next_url)</span><br><span class="line">        yield scrapy.Request(url&#x3D;response.url + max_id,</span><br><span class="line">                             callback&#x3D;self.parse_comments_next,</span><br><span class="line">                             dont_filter&#x3D;True)</span><br><span class="line"></span><br><span class="line">    def parse_comments_next(self, response):</span><br><span class="line">        try:</span><br><span class="line">            js &#x3D; json.loads(response.text)</span><br><span class="line"></span><br><span class="line">            for comment in js[&#39;data&#39;][&#39;data&#39;]:</span><br><span class="line">                comment_time &#x3D; comment[&#39;created_at&#39;]</span><br><span class="line">                text &#x3D; comment[&#39;text&#39;]</span><br><span class="line">                comment_people_id &#x3D; comment[&#39;user&#39;][&#39;id&#39;]</span><br><span class="line">                comment_people_name &#x3D; comment[&#39;user&#39;][&#39;screen_name&#39;]</span><br><span class="line">                comment_likes &#x3D; comment[&#39;like_count&#39;]</span><br><span class="line">                total_number &#x3D; comment[&#39;total_number&#39;]</span><br><span class="line">                item &#x3D; CommentItem(comment_time&#x3D;comment_time,</span><br><span class="line">                                text&#x3D;text,</span><br><span class="line">                                comment_people_id&#x3D;comment_people_id,</span><br><span class="line">                                comment_people_name&#x3D;comment_people_name,</span><br><span class="line">                                comment_likes&#x3D;comment_likes,</span><br><span class="line">                                total_number&#x3D;total_number)</span><br><span class="line">                yield item</span><br><span class="line">            max_id &#x3D; &quot;&amp;max_id&#x3D;&quot; + str(js[&#39;data&#39;][&#39;max_id&#39;])</span><br><span class="line">            max_id_type &#x3D; &#39;&amp;max_id_type&#x3D;&#39; + str(js[&#39;data&#39;][&#39;max_id_type&#39;])</span><br><span class="line">            print(&quot;&#x3D;&quot; * 40)</span><br><span class="line">            print(max_id)</span><br><span class="line">            print(max_id_type)</span><br><span class="line">            print(&quot;&#x3D;&quot; * 40)</span><br><span class="line">            yield scrapy.Request(url&#x3D;self.comments_url + max_id + max_id_type,</span><br><span class="line">                                callback&#x3D;self.parse_comments_next,</span><br><span class="line">                                dont_filter&#x3D;True)</span><br><span class="line">        except Exception as ret:</span><br><span class="line">            print(&quot;&#x3D;&quot; * 40)</span><br><span class="line">            print(&quot;此处出错！%s&quot; % ret)</span><br><span class="line">            print(response.text)</span><br><span class="line">            print(&quot;&#x3D;&quot; * 40)</span><br></pre></td></tr></table></figure><p>这份爬虫代码，已经将爬取用户微博主页和爬取微博详情页结合了起来，能够实现爬取一个微博用户的所有微博和他所有微博的所有评论功能。</p><p>具体的实现涉及到了scrapy框架的应用，利用callback不断跳转处理函数来实现处理不同的信息以及拼凑和传递不同的URL。</p><p>同时里面有一些看起来无用的调试代码，能让我在运行scrapy爬虫的时候清楚的看到哪里错了，除了什么问题等。</p><h4 id="5-5-编写pipelines-py"><a href="#5-5-编写pipelines-py" class="headerlink" title="5.5 编写pipelines.py"></a>5.5 编写pipelines.py</h4><ul><li>根据传入进来的item类不同，将信息放入不同的json文件夹里面去</li><li>使用了scrapy内置的json导出类<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">from weibo.items import PeopleItem, StatusesItem, CommentItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">from scrapy.exporters import JsonLinesItemExporter</span><br><span class="line">class WeiboPipeline(object):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.comments_fp &#x3D; open(&quot;comments.json&quot;, &quot;wb&quot;)</span><br><span class="line">        self.people_fp &#x3D; open(&#39;people.json&#39;, &#39;wb&#39;)</span><br><span class="line">        self.statuses_fp &#x3D; open(&#39;statuses.json&#39;, &#39;wb&#39;)</span><br><span class="line">        self.comments_exporter &#x3D; JsonLinesItemExporter(self.comments_fp,</span><br><span class="line">                                              ensure_ascii&#x3D;False)</span><br><span class="line">        self.people_exporter &#x3D; JsonLinesItemExporter(self.people_fp,</span><br><span class="line">                                              ensure_ascii&#x3D;False)</span><br><span class="line">        self.statuses_exporter &#x3D; JsonLinesItemExporter(self.statuses_fp,</span><br><span class="line">                                              ensure_ascii&#x3D;False)</span><br><span class="line">    </span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        if isinstance(item, CommentItem):</span><br><span class="line">            self.comments_exporter.export_item(item)</span><br><span class="line">        elif isinstance(item, PeopleItem):</span><br><span class="line">            self.people_exporter.export_item(item)</span><br><span class="line">        else:</span><br><span class="line">            self.statuses_exporter.export_item(item)</span><br><span class="line">        </span><br><span class="line">        return item</span><br><span class="line"></span><br><span class="line">    def close_item(self, spider):</span><br><span class="line">        print(&quot;存储成功！&quot;)</span><br><span class="line">        self.comments_fp.close()</span><br><span class="line">        self.people_fp.close()</span><br><span class="line">        self.statuses_fp.close()</span><br></pre></td></tr></table></figure></li></ul><h4 id="5-6-编写midelewares-py"><a href="#5-6-编写midelewares-py" class="headerlink" title="5.6 编写midelewares.py"></a>5.6 编写midelewares.py</h4><ul><li>实现自动更换请求头</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class WeiboDownloaderMiddleware(object):</span><br><span class="line">    # Not all methods need to be defined. If a method is not defined,</span><br><span class="line">    # scrapy acts as if the downloader middleware does not modify the</span><br><span class="line">    # passed objects.</span><br><span class="line">    user_agents &#x3D; [</span><br><span class="line">            &#39;Mozilla&#x2F;5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit&#x2F;534.50 (KHTML, like Gecko) Version&#x2F;5.1 Safari&#x2F;534.50&#39;,</span><br><span class="line">            &#39;Mozilla&#x2F;5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit&#x2F;534.50 (KHTML, like Gecko) Version&#x2F;5.1 Safari&#x2F;534.50&#39;,</span><br><span class="line">            &#39;Mozilla&#x2F;5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident&#x2F;5.0;&#39;,</span><br><span class="line">            &#39;Mozilla&#x2F;4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident&#x2F;4.0)&#39;,</span><br><span class="line">            &#39;Mozilla&#x2F;5.0 (Macintosh; Intel Mac OS X 10.6; rv,2.0.1) Gecko&#x2F;20100101 Firefox&#x2F;4.0.1&#39;,</span><br><span class="line">            &#39;Mozilla&#x2F;5.0 (Windows NT 6.1; rv,2.0.1) Gecko&#x2F;20100101 Firefox&#x2F;4.0.1&#39;,</span><br><span class="line">            &#39;Mozilla&#x2F;5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit&#x2F;535.11 (KHTML, like Gecko) Chrome&#x2F;17.0.963.56 Safari&#x2F;535.11&#39;</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">    def process_request(self, request, spider):</span><br><span class="line">        user_agent &#x3D; random.choice(self.user_agents)</span><br><span class="line">        request.headers[&#39;User-Agent&#39;] &#x3D; user_agent</span><br></pre></td></tr></table></figure><h3 id="六、-结果展示"><a href="#六、-结果展示" class="headerlink" title="六、 结果展示"></a>六、 结果展示</h3><h4 id="6-1-评论数据展示"><a href="#6-1-评论数据展示" class="headerlink" title="6.1 评论数据展示"></a>6.1 评论数据展示</h4><p><img src="https://img-blog.csdnimg.cn/20200305155940407.png" alt="评论数据展示"></p><h4 id="6-2-微博数据展示"><a href="#6-2-微博数据展示" class="headerlink" title="6.2 微博数据展示"></a>6.2 微博数据展示</h4><p><img src="https://img-blog.csdnimg.cn/20200305160044440.png" alt="微博数据展示"></p><h3 id="七、-项目总结"><a href="#七、-项目总结" class="headerlink" title="七、 项目总结"></a>七、 项目总结</h3><ul><li><p>这次微博数据爬取，对我自己也是一个不小的挑战，刚开始并没有使用mitmweb来抓包分析请求，一直在用jupyter和requests来不断更改请求头来确认需要的值和优化请求，经常会碰到请求数据失败和重定向而导致请求不到数据的问题。</p></li><li><p>同时，微博的这种前一个请求中带有后一个请求需要的参数这种请求方式刚开始也让我很懵逼，摸不着头脑。</p></li><li><p>与上文提到的一样，就在我以为成功了的时候，那个max_id_type着实坑了我一把，我想当然的以为这个值恒为0，没算到它居然会变。</p></li><li><p>接下来想去破解js加密的一些内容和登录的内容，然后去尝试抓取手机app的信息。</p></li></ul><p><strong>我是落阳，一个正在努力的无名之辈，谢谢你的关注。欢迎找我一起探讨问题。</strong></p><p>关注【程序小员】后台发送【微博爬虫】获取全部源码。</p><div id="reward-btn">打赏</div></div><footer class="article-footer"><a data-url="http://hsluoyang.club/python%E7%88%AC%E8%99%AB-%E5%B8%A6%E4%BD%A0%E6%B8%B8%E8%A7%88%E5%BE%AE%E5%8D%9A%E5%8D%9A%E4%B8%BB%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F.html" data-id="ck7r5r6cw0001fgvo4d5i302h" class="article-share-link">分享</a><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a></li></ul></footer></div><nav class="article-nav"><a href="/%E8%BD%BB%E6%9D%BE%EF%BC%8C%E9%AB%98%E6%95%88%EF%BC%8C%E6%AD%A3%E7%A1%AE%E7%9A%84%E5%86%99%E5%87%BA%E4%B8%80%E4%B8%AA%E7%88%AC%E8%99%AB%EF%BC%9F%E7%9C%8B%E8%BF%99%E4%B8%80%E7%AF%87%E5%B0%B1%E5%A4%9F%E4%BA%86%EF%BC%81%E8%B8%A9%E8%BF%87%E6%97%A0%E6%95%B0%E5%9D%91%E5%90%8E%E6%80%BB%E7%BB%93%E7%9A%84%E7%88%AC%E8%99%AB%E7%BC%96%E5%86%99%E6%B5%81%E7%A8%8B.html" class="article-nav-link"><strong class="article-nav-caption">上一篇</strong><div class="article-nav-title">轻松，高效，正确的写出一个爬虫？看这一篇就够了！</div></a><a href="/%E4%B8%80%E6%95%B4%E4%B8%AA%E7%BD%91%E7%AB%99%E7%9A%84%E5%85%A8%E9%83%A8%E6%95%B0%E6%8D%AE%EF%BC%8C%E6%88%91%E5%8F%AA%E8%83%BD%E7%BB%99%E4%BD%A0%E8%BF%99%E4%B9%88%E5%A4%9A%E4%BA%86%E3%80%82.html" class="article-nav-link"><strong class="article-nav-caption">下一篇</strong><div class="article-nav-title">一整个网站的全部数据，我只能给你这么多了。</div></a></nav><div id="vcomments-box"><div id="vcomments"></div></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js"></script><script>new Valine({
        el: '#vcomments',
        notify: false,
        verify: false,
        app_id: '5WSaK6bUDnRLhgbQjaL5R9Tq-gzGzoHsz',
        app_key: '7x0vYeAw0aRYU5SjaFugWDxh',
        path: window.location.pathname,
        avatar: 'mp',
        placeholder: '给我的文章加点评论吧~',
        recordIP: true
    });
    const infoEle = document.querySelector('#vcomments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
        infoEle.childNodes.forEach(function (item) {
            item.parentNode.removeChild(item);
        });
    }</script><style>#vcomments-box{padding:5px 30px}@media screen and (max-width:800px){#vcomments-box{padding:5px 0}}#vcomments-box #vcomments{background-color:#fff}.v .vlist .vcard .vh{padding-right:20px}.v .vlist .vcard{padding-left:10px}</style></article></section><footer class="footer"><div class="outer"><ul class="list-inline"><li>&copy; 2019-2020 落阳</li><li><a href="https://hexo.io" target="_blank">Hexo</a> Theme <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a> by shenyu</li></ul><ul class="list-inline"><li><ul class="list-inline"><li>PV:<span id="busuanzi_value_page_pv"></span></li><li>UV:<span id="busuanzi_value_site_uv"></span></li></ul></li><li></li></ul></div></footer><div class="to_top"><div class="totop" id="totop"><i class="ri-arrow-up-line"></i></div></div></main><aside class="sidebar"><button class="navbar-toggle"></button><nav class="navbar"><div class="logo"><a href="/"><img src="/images/touxiang.png" alt="落阳的博客"></a></div><ul class="nav nav-main"><li class="nav-item"><a class="nav-item-link" href="/">主页</a></li><li class="nav-item"><a class="nav-item-link" href="/archives">目录</a></li><li class="nav-item"><a class="nav-item-link" href="/tags">标签</a></li></ul></nav><nav class="navbar navbar-bottom"><ul class="nav"><li class="nav-item"><a class="nav-item-link nav-item-search" title="搜索"><i class="ri-search-line"></i> </a><a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed"><i class="ri-rss-line"></i></a></li></ul></nav><div class="search-form-wrap"><div class="local-search local-search-plugin"><input type="search" id="local-search-input" class="local-search-input" placeholder="Search..."><div id="local-search-result" class="local-search-result"></div></div></div></aside><div id="mask"></div><div id="reward"><span class="close"><i class="ri-close-line"></i></span><p class="reward-p"><i class="ri-cup-line"></i>谢谢你请我吃糖果~</p><div class="reward-box"><div class="reward-item"><img class="reward-img" src="/images/alipay.png"> <span class="reward-type">支付宝</span></div><div class="reward-item"><img class="reward-img" src="/images/weixin.png"> <span class="reward-type">微信</span></div></div></div><script src="/js/jquery-2.0.3.min.js"></script><script src="/js/jquery.justifiedGallery.min.js"></script><script src="/js/lazyload.min.js"></script><script src="/js/busuanzi-2.3.pure.min.js"></script><script src="/fancybox/jquery.fancybox.min.js"></script><script src="/js/tocbot.min.js"></script><script>tocbot.init({tocSelector:".tocbot",contentSelector:".article-entry",headingSelector:"h1, h2, h3, h4, h5, h6",hasInnerContainers:!0,scrollSmooth:!0,positionFixedSelector:".tocbot",positionFixedClass:"is-position-fixed",fixedSidebarOffset:"auto"})</script><script>var ayerConfig={mathjax:!1}</script><script src="/js/ayer.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css"><script type="text/javascript" src="https://js.users.51.la/20544303.js"></script></div></body>